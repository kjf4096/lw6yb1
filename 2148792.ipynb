{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "[æˆ‘åœ¨AI Studioä¸Šè·å¾—é»„é‡‘ç­‰çº§ï¼Œç‚¹äº®9ä¸ªå¾½ç« ï¼Œæ¥äº’å…³å‘€~](http://aistudio.baidu.com/aistudio/personalcenter/thirdview/335435) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# ï¼ˆ1ï¼‰èµ›é¢˜ä»‹ç»\n",
    "\n",
    "å›¾ç¥ç»ç½‘ç»œï¼ˆGraph Neural Networkï¼‰æ˜¯ä¸€ç§ä¸“é—¨å¤„ç†å›¾ç»“æ„æ•°æ®çš„ç¥ç»ç½‘ç»œï¼Œç›®å‰è¢«å¹¿æ³›åº”ç”¨äºæ¨èç³»ç»Ÿã€é‡‘èé£æ§ã€ç”Ÿç‰©è®¡ç®—ç­‰é¢†åŸŸã€‚å›¾ç¥ç»ç½‘ç»œçš„ç»å…¸é—®é¢˜ä¸»è¦æœ‰ä¸‰ç±»ï¼Œåˆ†åˆ«ä¸ºèŠ‚ç‚¹åˆ†ç±»ã€è¿æ¥é¢„æµ‹å’Œå›¾åˆ†ç±»ã€‚æœ¬æ¬¡æ¯”èµ›æ—¨åœ¨è®©å‚èµ›åŒå­¦äº†è§£å¹¶æŒæ¡å¦‚ä½•ä½¿ç”¨å›¾ç¥ç»ç½‘ç»œå¤„ç†èŠ‚ç‚¹åˆ†ç±»é—®é¢˜ã€‚\n",
    "\n",
    "åœ¨è¿‡å»çš„ä¸€ä¸ªä¸–çºªé‡Œï¼Œç§‘å­¦å‡ºç‰ˆç‰©çš„æ•°é‡æ¯12å¹´å¢åŠ è¿‘ä¸€å€ï¼Œå¯¹æ¯ä¸€ç§å‡ºç‰ˆç‰©çš„ä¸»é¢˜åŠé¢†åŸŸè¿›è¡Œè‡ªåŠ¨åˆ†ç±»å·²æˆä¸ºå½“ä¸‹ååˆ†é‡è¦çš„å·¥ä½œã€‚æœ¬æ¬¡ä»»åŠ¡çš„ç›®æ ‡æ˜¯é¢„æµ‹æœªçŸ¥è®ºæ–‡çš„ä¸»é¢˜ç±»åˆ«ï¼Œå¦‚è½¯ä»¶å·¥ç¨‹ï¼Œäººå·¥æ™ºèƒ½ï¼Œè¯­è¨€è®¡ç®—å’Œæ“ä½œç³»ç»Ÿç­‰ã€‚æ¯”èµ›æ‰€é€‰35ä¸ªé¢†åŸŸæ ‡ç­¾å·²å¾—åˆ°è®ºæ–‡ä½œè€…å’ŒarXivç‰ˆä¸»ç¡®è®¤å¹¶æ ‡è®°ã€‚\n",
    "\n",
    "æœ¬æ¬¡æ¯”èµ›é€‰ç”¨çš„æ•°æ®é›†ä¸ºarXivè®ºæ–‡å¼•ç”¨ç½‘ç»œâ€”â€”ogbn-arixvæ•°æ®é›†çš„å­é›†ã€‚ogbn-arixvæ•°æ®é›†ç”±å¤§é‡çš„å­¦æœ¯è®ºæ–‡ç»„æˆï¼Œè®ºæ–‡ä¹‹é—´çš„å¼•ç”¨å…³ç³»å½¢æˆä¸€å¼ å·¨å¤§çš„æœ‰å‘å›¾ï¼Œæ¯ä¸€æ¡æœ‰å‘è¾¹è¡¨ç¤ºä¸€ç¯‡è®ºæ–‡å¼•ç”¨å¦ä¸€ç¯‡è®ºæ–‡ï¼Œæ¯ä¸€ä¸ªèŠ‚ç‚¹æä¾›100ç»´ç®€å•çš„è¯å‘é‡ä½œä¸ºèŠ‚ç‚¹ç‰¹å¾ã€‚åœ¨è®ºæ–‡å¼•ç”¨ç½‘ç»œä¸­ï¼Œæˆ‘ä»¬å·²å¯¹è®­ç»ƒé›†å¯¹åº”èŠ‚ç‚¹åšäº†è®ºæ–‡ç±»åˆ«æ ‡æ³¨å¤„ç†ã€‚æœ¬æ¬¡ä»»åŠ¡å¸Œæœ›å‚èµ›è€…é€šè¿‡å·²æœ‰çš„èŠ‚ç‚¹ç±»åˆ«ä»¥åŠè®ºæ–‡ä¹‹é—´çš„å¼•ç”¨å…³ç³»ï¼Œé¢„æµ‹æœªçŸ¥èŠ‚ç‚¹çš„è®ºæ–‡ç±»åˆ«ã€‚\n",
    "\n",
    "\n",
    "[èµ›é¢˜åœ°å€](https://aistudio.baidu.com/aistudio/competition/detail/59)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# ï¼ˆ2ï¼‰é‡è¦å‚è€ƒ\n",
    "###  1.UniMPç®—æ³•\n",
    "[UniMPç®—æ³•GitHubé“¾æ¥](https://github.com/PaddlePaddle/PGL/tree/main/ogb_examples/nodeproppred/unimp)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 2.å‚è€ƒä»£ç \n",
    "[https://aistudio.baidu.com/aistudio/projectdetail/1467127?channelType=0&channel=0](https://aistudio.baidu.com/aistudio/projectdetail/1467127?channelType=0&channel=0)\n",
    "\n",
    "[é£æ¡¨å¸¸è§„èµ›ï¼šå›¾ç¥ç»ç½‘ç»œå…¥é—¨èŠ‚ç‚¹åˆ†ç±» 5æœˆç¬¬4åæ–¹æ¡ˆ](https://aistudio.baidu.com/aistudio/projectdetail/1931047)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# ï¼ˆ3ï¼‰å…·ä½“æ–¹æ¡ˆåˆ†äº«"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## ç¯å¢ƒé…ç½®\n",
    "\n",
    "```\n",
    "\n",
    "#å¯¼å…¥ç›¸å…³åŒ…\n",
    "!pip install --upgrade python-dateutil\n",
    "!pip install easydict\n",
    "!pip install pgl==1.2.0 \n",
    "!pip install pandas>=0.25\n",
    "!pip install pyarrow==0.13.0\n",
    "!pip install chardet==3.0.4\n",
    "\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "# å°è¯•ä½¿ç”¨æ ‡ç­¾è¿›è¡Œè®­ç»ƒï¼š\n",
    "ï¼ˆè¯•éªŒç»“æœï¼šè¿‡æ‹Ÿåˆä¸¥é‡ï¼Œä½†å¯ä»¥ä¸ºæœ€åæŠ•ç¥¨é›†æˆæä¾›æ•°æ®ï¼Œä»£ç ä»…ä¾›å‚è€ƒï¼‰\n",
    "### â‘ ä»è®­ç»ƒé›†ä¸­éšæœºé€‰æ‹©35ä¸ªç±»åˆ«å„1ä¸ª\n",
    "### â‘¡åˆ©ç”¨ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—æ¯ä¸ªfeatå¯èƒ½çš„ç±»åˆ«\n",
    "### â‘¢æŠŠæ¯ä¸ªfeatä¸­ç”¨ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—å‡ºçš„æ ‡ç­¾ç±»åˆ«çš„ä½ç½®è®¾ç½®æˆ1ï¼Œå…¶ä½™ä½ç½®è®¾ç½®ä¸º0\n",
    "### â‘£ç”¨æ–°çš„featè¿›è¡Œè®­ç»ƒ\n",
    "### ä»£ç å¦‚ä¸‹ï¼š\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "edges = pd.read_csv(\"work/edges.csv\", header=None, names=[\"src\", \"dst\"]).values\n",
    "node_feat = np.load(\"work/feat.npy\")\n",
    "df = pd.read_csv(\"work/train.csv\")\n",
    "node_index = df[\"nid\"].values\n",
    "node_label = df[\"label\"].values\n",
    "feat_dict={}\n",
    "feat1=np.zeros([node_feat.shape[0],100],dtype='float32')\n",
    "for i in range(len(node_feat)):\n",
    "    feat_dict[i]=0\n",
    "for i in range(len(node_index)):\n",
    "    if i<int(0.8*len(node_index)):\n",
    "        feat_dict[node_index[i]]=1\n",
    "    else:\n",
    "        feat_dict[node_index[i]]=2\n",
    "node_labels={}\n",
    "for i in range(len(node_index)):\n",
    "    node_labels[node_index[i]]=node_label[i]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def cos_sim(vector_a, vector_b):\n",
    "    \"\"\"\n",
    "    è®¡ç®—ä¸¤ä¸ªå‘é‡ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦\n",
    "     :param vector_a: å‘é‡ a\n",
    "     :param vector_b: å‘é‡ b\n",
    "    :return: sim\n",
    "    \"\"\"\n",
    "    vector_a = np.mat(vector_a)\n",
    "    vector_b = np.mat(vector_b)\n",
    "    num = float(vector_a * vector_b.T)\n",
    "    denom = np.linalg.norm(vector_a) * np.linalg.norm(vector_b)\n",
    "    cos = num / denom\n",
    "    sim = 0.5 + 0.5 * cos\n",
    "    return sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "rnd_list=[]\n",
    "\n",
    "for j in range(35):\n",
    "    while True:\n",
    "        n=np.random.randint(0,len(node_index))\n",
    "        if node_label[n]==j:\n",
    "            rnd_list.append(n)      \n",
    "            break      \n",
    "\n",
    "\n",
    "for i in range(len(node_feat)):\n",
    "    \n",
    "    if feat_dict[i]!=1:\n",
    "        cos_sim_max=0\n",
    "        j_max=0\n",
    "        for j in range(len(rnd_list)):\n",
    "            sim=cos_sim(node_feat[i],node_feat[rnd_list[j]])\n",
    "            if sim>cos_sim_max:\n",
    "                com_sim_max=sim\n",
    "                j_max=j\n",
    "            if cos_sim_max>0.95:\n",
    "                break\n",
    "        feat1[i,node_label[rnd_list[j_max]]]=1.0\n",
    "    else:\n",
    "        feat1[i,node_labels[i]]=1.0\n",
    "    if i%10000==0:\n",
    "        print(i)\n",
    "node_feat=feat1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# ï¼ˆ4ï¼‰ä»£ç å®ç°\n",
    "\n",
    "###  1.æ¨¡å‹æ„å»ºï¼ˆmode.pyï¼‰\n",
    "Res_Unimp_Largeä»£ç ï¼Œä¹Ÿå¯å‚è§model_modified.pyã€‚\n",
    "\n",
    "```\n",
    "class res_unimp_large(object):\n",
    "    def __init__(self, config, num_class):\n",
    "        self.num_class = num_class\n",
    "        self.num_layers = config.get(\"num_layers\", 2)\n",
    "        self.hidden_size = config.get(\"hidden_size\", 128)\n",
    "        self.out_size=config.get(\"out_size\", 40)\n",
    "        self.embed_size=config.get(\"embed_size\", 100)\n",
    "        self.heads = config.get(\"heads\", 8) \n",
    "        self.dropout = config.get(\"dropout\", 0.3)\n",
    "        self.edge_dropout = config.get(\"edge_dropout\", 0.0)\n",
    "        self.use_label_e = config.get(\"use_label_e\", False)\n",
    "    \n",
    "    # ç¼–ç è¾“å…¥        \n",
    "    def embed_input(self, feature):   \n",
    "        lay_norm_attr=F.ParamAttr(initializer=F.initializer.ConstantInitializer(value=1))\n",
    "        lay_norm_bias=F.ParamAttr(initializer=F.initializer.ConstantInitializer(value=0))\n",
    "        feature=L.layer_norm(feature, name='layer_norm_feature_input', \n",
    "                                      param_attr=lay_norm_attr, \n",
    "                                      bias_attr=lay_norm_bias)\n",
    "        return feature\n",
    "    \n",
    "    # è¿åŒéƒ¨åˆ†å·²çŸ¥çš„æ ‡ç­¾ç¼–ç è¾“å…¥ï¼ˆMaskLabelï¼‰\n",
    "    def label_embed_input(self, feature):\n",
    "        label = F.data(name=\"label\", shape=[None, 1], dtype=\"int64\")\n",
    "        label_idx = F.data(name='label_idx', shape=[None, 1], dtype=\"int64\")\n",
    "\n",
    "        label = L.reshape(label, shape=[-1])\n",
    "        label_idx = L.reshape(label_idx, shape=[-1])\n",
    "\n",
    "        embed_attr = F.ParamAttr(initializer=F.initializer.NormalInitializer(loc=0.0, scale=1.0))\n",
    "        embed = F.embedding(input=label, size=(self.out_size, self.embed_size), param_attr=embed_attr )\n",
    "\n",
    "        feature_label = L.gather(feature, label_idx, overwrite=False)\n",
    "        feature_label = feature_label + embed\n",
    "        feature = L.scatter(feature, label_idx, feature_label, overwrite=True)\n",
    "     \n",
    "        lay_norm_attr = F.ParamAttr(initializer=F.initializer.ConstantInitializer(value=1))\n",
    "        lay_norm_bias = F.ParamAttr(initializer=F.initializer.ConstantInitializer(value=0))\n",
    "        feature = L.layer_norm(feature, name='layer_norm_feature_input', \n",
    "                                      param_attr=lay_norm_attr, \n",
    "                                      bias_attr=lay_norm_bias)\n",
    "        return feature\n",
    "        \n",
    "    def forward(self, graph_wrapper, feature, phase):\n",
    "        if phase == \"train\": \n",
    "            edge_dropout = self.edge_dropout\n",
    "            dropout = self.dropout\n",
    "        else:\n",
    "            edge_dropout = 0\n",
    "            dropout = 0\n",
    "\n",
    "        if self.use_label_e:\n",
    "            feature = self.label_embed_input(feature)\n",
    "        else:\n",
    "            feature = self.embed_input(feature)\n",
    "        if dropout > 0:\n",
    "            feature = L.dropout(feature, dropout_prob=dropout, \n",
    "                                    dropout_implementation='upscale_in_train')\n",
    "        \n",
    "        #æ”¹å˜è¾“å…¥ç‰¹å¾ç»´åº¦æ˜¯ä¸ºäº†Resè¿æ¥å¯ä»¥ç›´æ¥ç›¸åŠ \n",
    "        feature = L.fc(feature, size=self.hidden_size * self.heads, name=\"init_feature\")\n",
    "\n",
    "\n",
    "        for i in range(self.num_layers - 1):\n",
    "            ngw = pgl.sample.edge_drop(graph_wrapper, edge_dropout) \n",
    "            from model_unimp_large import graph_transformer, attn_appnp\n",
    "\n",
    "            res_feature = feature\n",
    "\n",
    "            feature, _, cks = graph_transformer(str(i), ngw, feature, \n",
    "                                             hidden_size=self.hidden_size,\n",
    "                                             num_heads=self.heads, \n",
    "                                             concat=True, skip_feat=True,\n",
    "                                             layer_norm=True, relu=True, gate=True)\n",
    "            if dropout > 0:\n",
    "                feature = L.dropout(feature, dropout_prob=dropout, \n",
    "                                     dropout_implementation='upscale_in_train') \n",
    "            \n",
    "            # ä¸‹é¢è¿™è¡Œä¾¿æ˜¯Resè¿æ¥äº†\n",
    "            feature = res_feature + feature \n",
    "        \n",
    "        feature, attn, cks = graph_transformer(str(self.num_layers - 1), ngw, feature, \n",
    "                                             hidden_size=self.out_size,\n",
    "                                             num_heads=self.heads, \n",
    "                                             concat=False, skip_feat=True,\n",
    "                                             layer_norm=False, relu=False, gate=True)\n",
    "\n",
    "        feature = attn_appnp(ngw, feature, attn, alpha=0.2, k_hop=10)\n",
    "\n",
    "        pred = L.fc(\n",
    "            feature, self.num_class, act=None, name=\"pred_output\")\n",
    "        return pred\n",
    "```\n",
    "        \n",
    "### 2.æ¨¡å‹é…ç½®ï¼ˆNotebookï¼‰\n",
    "æœ€ä¼˜ç­–ç•¥ï¼š3å±‚res_unimp_largeï¼Œéšå±‚ç¥ç»å…ƒ128ä¸ªï¼Œé…ç½®ä¸¤ç§dropoutï¼Œä½¿ç”¨MaskLabelï¼Œä¸”label_rate = 0.66ï¼ˆåœ¨æ¨¡å‹è®­ç»ƒä¸­è®¾ç½®ï¼‰ã€‚\n",
    "\n",
    "```\n",
    "config = {\n",
    "    \"model_name\": \"res_unimp_large\",\n",
    "    \"num_layers\": 3,\n",
    "    \"hidden_size\": 128,\n",
    "    \"heads\": 2,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"dropout\": 0.33,\n",
    "    \"weight_decay\": 0.0005,\n",
    "    \"edge_dropout\": 0.32,\n",
    "    \"use_label_e\": True\n",
    "}\n",
    "\n",
    "```\n",
    "###  3.æ¨¡å‹è®­ç»ƒï¼ˆNotebookï¼‰\n",
    "\n",
    "```\n",
    "import os\n",
    "use_label_e = True\n",
    "label_rate = 0.66\n",
    "epoch = 4000\n",
    "exe.run(startup_program)\n",
    "max_val_acc = 0\n",
    "\n",
    "# è¿™é‡Œå¯ä»¥æ¢å¤è®­ç»ƒ\n",
    "pretrained = False\n",
    "if pretrained:\n",
    "    def name_filter(var):\n",
    "        res = var.name in os.listdir('./output')\n",
    "        return res\n",
    "    fluid.io.load_vars(exe, './output',predicate=name_filter)\n",
    "    max_val_acc = 0.756\n",
    "\n",
    "earlystop = 0\n",
    "# å°†å›¾æ•°æ®å˜æˆ feed_dict ç”¨äºä¼ å…¥Paddle Excecutor\n",
    "feed_dict = gw.to_feed(dataset.graph)\n",
    "for epoch in range(epoch):\n",
    "    # Full Batch è®­ç»ƒ\n",
    "    # è®¾å®šå›¾ä¸Šé¢é‚£äº›èŠ‚ç‚¹è¦è·å–\n",
    "    # node_index: æœªçŸ¥labelèŠ‚ç‚¹çš„nid    \n",
    "    # node_label: æœªçŸ¥label\n",
    "    # label_idx: å·²çŸ¥labelèŠ‚ç‚¹çš„nid    \n",
    "    # label: å·²çŸ¥label\n",
    "    \n",
    "    if use_label_e:\n",
    "        # åœ¨è®­ç»ƒé›†ä¸­æŠ½å–éƒ¨åˆ†æ•°æ®ï¼Œå…¶Labelå·²çŸ¥ï¼Œå¹¶å¯ä»¥è¾“å…¥ç½‘ç»œè®­ç»ƒ\n",
    "        train_idx_temp = np.array(train_index, dtype=\"int64\")\n",
    "        train_lab_temp = np.array(train_label, dtype=\"int64\")\n",
    "        state = np.random.get_state()\n",
    "        np.random.shuffle(train_idx_temp)\n",
    "        np.random.set_state(state)\n",
    "        np.random.shuffle(train_lab_temp)\n",
    "\n",
    "        label_idx=train_idx_temp[:int(label_rate*len(train_idx_temp))]\n",
    "        unlabel_idx=train_idx_temp[int(label_rate*len(train_idx_temp)):]\n",
    "        label=train_lab_temp[:int(label_rate*len(train_idx_temp))]\n",
    "        unlabel=train_lab_temp[int(label_rate*len(train_idx_temp)):]\n",
    "\n",
    "        feed_dict[\"node_index\"] = unlabel_idx\n",
    "        feed_dict[\"node_label\"] = unlabel\n",
    "        feed_dict['label_idx']= label_idx\n",
    "        feed_dict['label']= label\n",
    "    else:\n",
    "        feed_dict[\"node_label\"] = np.array(train_label, dtype=\"int64\")\n",
    "        feed_dict[\"node_index\"] = np.array(train_index, dtype=\"int64\")\n",
    "        \n",
    "\n",
    "    train_loss, train_acc = exe.run(train_program,\n",
    "                                feed=feed_dict,\n",
    "                                fetch_list=[loss, acc],\n",
    "                                return_numpy=True)\n",
    "\n",
    "    # Full Batch éªŒè¯\n",
    "    # è®¾å®šå›¾ä¸Šé¢é‚£äº›èŠ‚ç‚¹è¦è·å–\n",
    "    # node_index: æœªçŸ¥labelèŠ‚ç‚¹çš„nid    \n",
    "    # node_label: æœªçŸ¥label\n",
    "    # label_idx: å·²çŸ¥labelèŠ‚ç‚¹çš„nid    \n",
    "    # label: å·²çŸ¥label\n",
    "    \n",
    "    feed_dict[\"node_index\"] = np.array(val_index, dtype=\"int64\")\n",
    "    feed_dict[\"node_label\"] = np.array(val_label, dtype=\"int64\")\n",
    "    if use_label_e:\n",
    "        feed_dict['label_idx'] = np.array(train_index, dtype=\"int64\")\n",
    "        feed_dict['label'] = np.array(train_label, dtype=\"int64\")\n",
    "    val_loss, val_acc = exe.run(test_program,\n",
    "                            feed=feed_dict,\n",
    "                            fetch_list=[v_loss, v_acc],\n",
    "                            return_numpy=True)\n",
    "    print(\"Epoch\", epoch, \"Train Acc\", train_acc[0], \"Valid Acc\", val_acc[0])\n",
    "    \n",
    "    # ä¿å­˜å†å²æœ€ä¼˜éªŒè¯ç²¾åº¦å¯¹åº”çš„æ¨¡å‹\n",
    "    if val_acc[0] > max_val_acc:\n",
    "        max_val_acc = val_acc[0]\n",
    "        fluid.io.save_persistables(exe, './output', train_program)\n",
    "    \n",
    "    # è®­ç»ƒç²¾åº¦æŒç»­å¤§äºéªŒè¯ç²¾åº¦ï¼Œç»“æŸè®­ç»ƒ\n",
    "    if train_acc[0] > val_acc[0]:\n",
    "        earlystop += 1\n",
    "        if earlystop == 40:\n",
    "            break\n",
    "    else:\n",
    "        earlystop = 0\n",
    "```\n",
    "\n",
    "### 4.ç®€å•æŠ•ç¥¨\n",
    "```\n",
    "def publicnum(num, d = 0):\n",
    "    dictnum = {}\n",
    "    for i in range(len(num)):\n",
    "        if num[i] in dictnum.keys():\n",
    "            dictnum[num[i]] += 1\n",
    "        else:\n",
    "            dictnum.setdefault(num[i], 1)\n",
    "    maxnum = 0\n",
    "    maxkey = 0\n",
    "    for k, v in dictnum.items():\n",
    "        if v > maxnum:\n",
    "            maxnum = v\n",
    "            maxkey = k\n",
    "    return maxkey\n",
    "\n",
    "df0=pd.read_csv(\"submission0.76136.csv\")\n",
    "df1=pd.read_csv(\"submission0.757822.csv\")\n",
    "df2=pd.read_csv(\"submission0.7583.csv\")\n",
    "df3=pd.read_csv(\"submission0.75758.csv\")\n",
    "df4=pd.read_csv(\"submission0.75921.csv\")\n",
    "df5=pd.read_csv(\"submission0.75782.csv\")\n",
    "df6=pd.read_csv(\"submission0.75956.csv\")\n",
    "df7=pd.read_csv(\"submission0.75801.csv\")\n",
    "df8=pd.read_csv(\"submission0.75884.csv\")\n",
    "#df9=pd.read_csv(\"submission9.csv\")\n",
    "#df10=pd.read_csv(\"submission10.csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "nids=[]\n",
    "labels=[]\n",
    "\n",
    "for i in range(df4.shape[0]):\n",
    "    label_zs=[]\n",
    "    label_zs.append(df0.label[i])\n",
    "    label_zs.append(df1.label[i])\n",
    "    label_zs.append(df2.label[i])\n",
    "    label_zs.append(df3.label[i])\n",
    "    label_zs.append(df4.label[i])\n",
    "    label_zs.append(df5.label[i])\n",
    "    label_zs.append(df6.label[i])\n",
    "    label_zs.append(df7.label[i])\n",
    "    label_zs.append(df8.label[i])\n",
    "    #label_zs.append(df9.label[i])\n",
    "    #label_zs.append(df10.label[i])\n",
    "    lab=publicnum(label_zs, d = 0)\n",
    "    labels.append(lab)\n",
    "    nids.append(df4.nid[i])\n",
    "\n",
    "\n",
    "submission = pd.DataFrame(data={\n",
    "                            \"nid\": nids,\n",
    "                            \"label\": labels\n",
    "                        })\n",
    "submission.to_csv(\"submissiona.csv\", index=False)\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# ï¼ˆ5ï¼‰æ€»ç»“åŠæ”¹å–„æ–¹å‘\r\n",
    "1. ä½¿ç”¨UniMPç®—æ³•å¯ä»¥æé«˜æˆç»©ã€‚\r\n",
    "2. æå‰ä¸­æ­¢æœ‰åˆ©äºå‡å°‘è¿‡æ‹Ÿåˆæé«˜æˆç»©ã€‚\r\n",
    "3. æŠ•ç¥¨æ–¹æ³•èƒ½æé«˜æˆç»©ï¼Œä½†æ˜¯å­˜åœ¨å¤©èŠ±æ¿ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# ï¼ˆ6ï¼‰ç»™å…¶ä»–é€‰æ‰‹å­¦ä¹ é£æ¡¨çš„å»ºè®®\n",
    "\n",
    "####  å»ºè®®å¤§å®¶å¤šå‚åŠ ç™¾åº¦AI Studioè¯¾ç¨‹ï¼Œå¤šçœ‹åˆ«äººå†™çš„AI Studioé¡¹ç›®ï¼Œä¹Ÿè®¸ä¼šæœ‰çµæ„Ÿè¿¸å‘ï¼Œåœ¨æ¯”èµ›ä¸­å–å¾—æ›´å¥½çš„æˆç»©ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# ï¼ˆ7ï¼‰One More Thing\n",
    "å¦‚æœå¤§å®¶è¿˜æƒ³è¦åˆ«çš„å¥‡æ€å¦™æƒ³ï¼Œå¯ä»¥å‚è€ƒä»¥ä¸‹è®ºæ–‡ï¼Œä»–ä»¬éƒ½åœ¨èŠ‚ç‚¹åˆ†ç±»ä¸Šæœ‰å¾ˆå¤§æå‡ã€‚\n",
    "\n",
    "[Predict then Propagate: Graph Neural Networks meet Personalized PageRank](https://arxiv.org/abs/1810.05997)\n",
    "\n",
    "[Simple and Deep Graph Convolutional Networks](https://arxiv.org/abs/2007.02133)\n",
    "\n",
    "[Masked Label Prediction: Unified Message Passing Model for Semi-Supervised Classification](https://arxiv.org/abs/2009.03509)\n",
    "\n",
    "[Combining Label Propagation and Simple Models Out-performs Graph Neural Networks](https://arxiv.org/abs/2010.13993)\n",
    "\n",
    "å¤§å®¶ä¹Ÿå¯ä»¥çœ‹çœ‹githubçš„[ UniMP](https://github.com/PaddlePaddle/PGL/tree/main/ogb_examples/nodeproppred/unimp)ç®—æ³• è¿™ä¸ªä¾‹å­ï¼Œé‡Œé¢æœ‰ç›¸ä¼¼çš„æ•°æ®é›†ï¼Œå¹¶ä¸”æœ€è¿‘ä¹Ÿæ˜¯SOTAæ•ˆæœï¼Œæœ‰å¸®åŠ©ğŸ‘æ¬¢è¿ç‚¹Star\n",
    "\n",
    "ç›¸å…³è¯¾ç¨‹è¿æ¥ï¼š[å›¾ç¥ç»ç½‘ç»œ7æ—¥æ‰“å¡è¥](http://aistudio.baidu.com/aistudio/education/group/info/1956)\n",
    "\n",
    "ä»£ç å‚è€ƒï¼š\n",
    "\n",
    "[è®ºæ–‡å¼•ç”¨ç½‘ç»œèŠ‚ç‚¹åˆ†ç±»â€”ç‚¼ä¸¹ç»éªŒæ€»ç»“](https://aistudio.baidu.com/aistudio/projectdetail/1642136)\n",
    "\n",
    "[é£æ¡¨å¸¸è§„èµ›ï¼šå›¾ç¥ç»ç½‘ç»œå…¥é—¨èŠ‚ç‚¹åˆ†ç±» 5æœˆç¬¬4åæ–¹æ¡ˆ](https://aistudio.baidu.com/aistudio/projectdetail/1931047)\n",
    "\n",
    "[æˆ‘åœ¨AI Studioä¸Šè·å¾—é»„é‡‘ç­‰çº§ï¼Œç‚¹äº®9ä¸ªå¾½ç« ï¼Œæ¥äº’å…³å‘€~](http://aistudio.baidu.com/aistudio/personalcenter/thirdview/335435) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## ä»£ç æ•´ä½“é€»è¾‘\n",
    "\n",
    "1. è¯»å–æä¾›çš„æ•°æ®é›†ï¼ŒåŒ…å«æ„å›¾ä»¥åŠè¯»å–èŠ‚ç‚¹ç‰¹å¾ï¼ˆç”¨æˆ·å¯è‡ªå·±æ”¹åŠ¨è¾¹çš„æ„é€ æ–¹å¼ï¼‰\n",
    "\n",
    "2. é…ç½®åŒ–ç”Ÿæˆæ¨¡å‹ï¼Œç”¨æˆ·ä¹Ÿå¯ä»¥æ ¹æ®æ•™ç¨‹è¿›è¡Œå›¾ç¥ç»ç½‘ç»œçš„å®ç°ã€‚\n",
    "\n",
    "3. å¼€å§‹è®­ç»ƒ\n",
    "\n",
    "4. æ‰§è¡Œé¢„æµ‹å¹¶äº§ç”Ÿç»“æœæ–‡ä»¶\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## ç¯å¢ƒé…ç½®\n",
    "\n",
    "è¯¥é¡¹ç›®ä¾èµ–é£æ¡¨paddlepaddle==1.8.4, ä»¥åŠpgl==1.2.0ã€‚è¯·æŒ‰ç…§ç‰ˆæœ¬å·ä¸‹è½½å¯¹åº”ç‰ˆæœ¬å°±å¯è¿è¡Œã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!unzip -oq /home/aistudio/data/data93851/graph.zip -d work/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#å¯¼å…¥ç›¸å…³åŒ…\r\n",
    "!pip install --upgrade python-dateutil\r\n",
    "!pip install easydict\r\n",
    "!pip install pgl==1.2.0 \r\n",
    "!pip install pandas>=0.25\r\n",
    "!pip install pyarrow==0.13.0\r\n",
    "!pip install chardet==3.0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pgl\n",
    "import paddle.fluid as fluid\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## å›¾ç½‘ç»œé…ç½®\n",
    "\n",
    "è¿™é‡Œå·²ç»æœ‰å¾ˆå¤šå¼ºå¤§çš„æ¨¡å‹é…ç½®äº†ï¼Œä½ å¯ä»¥å°è¯•ç®€å•çš„æ”¹ä¸€ä¸‹configçš„å­—æ®µã€‚\n",
    "ä¾‹å¦‚ï¼Œæ¢æˆGATçš„é…ç½®\n",
    "```\n",
    "config = {\n",
    "    \"model_name\": \"GAT\",\n",
    "    \"num_layers\":  1,\n",
    "    \"dropout\": 0.5,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"weight_decay\": 0.0005,\n",
    "    \"edge_dropout\": 0.00,\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from easydict import EasyDict as edict\n",
    "config = {\n",
    "    \"model_name\": \"res_unimp_large\",\n",
    "    \"num_layers\": 3,\n",
    "    \"hidden_size\": 128,\n",
    "    \"heads\": 2,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"dropout\": 0.33,\n",
    "    \"weight_decay\": 0.0005,\n",
    "    \"edge_dropout\": 0.32,\n",
    "    \"use_label_e\": True\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "config = edict(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## æ•°æ®åŠ è½½æ¨¡å—\n",
    "\n",
    "è¿™é‡Œä¸»è¦æ˜¯ç”¨äºè¯»å–æ•°æ®é›†ï¼ŒåŒ…æ‹¬è¯»å–å›¾æ•°æ®æ„å›¾ï¼Œä»¥åŠè®­ç»ƒé›†çš„åˆ’åˆ†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Dataset = namedtuple(\"Dataset\", \n",
    "               [\"graph\", \"num_classes\", \"train_index\",\n",
    "                \"train_label\", \"valid_index\", \"valid_label\", \"test_index\"])\n",
    "\n",
    "def load_edges(num_nodes, self_loop=True, add_inverse_edge=True):\n",
    "    # ä»æ•°æ®ä¸­è¯»å–è¾¹\n",
    "    edges = pd.read_csv(\"work/edges.csv\", header=None, names=[\"src\", \"dst\"]).values\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if add_inverse_edge:\n",
    "        edges = np.vstack([edges, edges[:, ::-1]])\n",
    "\n",
    "    if self_loop:\n",
    "        src = np.arange(0, num_nodes)\n",
    "        dst = np.arange(0, num_nodes)\n",
    "        self_loop = np.vstack([src, dst]).T\n",
    "        edges = np.vstack([edges, self_loop])\n",
    "    \n",
    "    return edges\n",
    "\n",
    "def load():\n",
    "    # ä»æ•°æ®ä¸­è¯»å–ç‚¹ç‰¹å¾å’Œè¾¹ï¼Œä»¥åŠæ•°æ®åˆ’åˆ†\n",
    "    node_feat = np.load(\"work/feat.npy\")\n",
    "    #node_feat=feat1\n",
    "    num_nodes = node_feat.shape[0]\n",
    "    edges = load_edges(num_nodes=num_nodes, self_loop=True, add_inverse_edge=True)\n",
    "    graph = pgl.graph.Graph(num_nodes=num_nodes, edges=edges, node_feat={\"feat\": node_feat})\n",
    "    \n",
    "    indegree = graph.indegree()\n",
    "    norm = np.maximum(indegree.astype(\"float32\"), 1)\n",
    "    norm = np.power(norm, -0.5)\n",
    "    graph.node_feat[\"norm\"] = np.expand_dims(norm, -1)\n",
    "    \n",
    "    df = pd.read_csv(\"work/train.csv\")\n",
    "    # æ‰“ä¹±é¡ºåº\n",
    "    df.sample(frac=1.0) \n",
    "    node_index = df[\"nid\"].values\n",
    "    node_label = df[\"label\"].values\n",
    "    train_part = int(len(node_index) * 0.8)\n",
    "    train_index = node_index[:train_part]\n",
    "    train_label = node_label[:train_part]\n",
    "    valid_index = node_index[train_part:]\n",
    "    valid_label = node_label[train_part:]\n",
    "    test_index = pd.read_csv(\"work/test.csv\")[\"nid\"].values\n",
    "    dataset = Dataset(graph=graph, \n",
    "                    train_label=train_label,\n",
    "                    train_index=train_index,\n",
    "                    valid_index=valid_index,\n",
    "                    valid_label=valid_label,\n",
    "                    test_index=test_index, num_classes=35)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = load()\n",
    "\n",
    "train_index = dataset.train_index\n",
    "train_label = np.reshape(dataset.train_label, [-1 , 1])\n",
    "train_index = np.expand_dims(train_index, -1)\n",
    "\n",
    "val_index = dataset.valid_index\n",
    "val_label = np.reshape(dataset.valid_label, [-1, 1])\n",
    "val_index = np.expand_dims(val_index, -1)\n",
    "\n",
    "test_index = dataset.test_index\n",
    "test_index = np.expand_dims(test_index, -1)\n",
    "test_label = np.zeros((len(test_index), 1), dtype=\"int64\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## ç»„ç½‘æ¨¡å—\n",
    "\n",
    "è¿™é‡Œæ˜¯ç»„ç½‘æ¨¡å—ï¼Œç›®å‰å·²ç»æä¾›äº†ä¸€äº›é¢„å®šä¹‰çš„æ¨¡å‹ï¼ŒåŒ…æ‹¬**GCN**, **GAT**, **APPNP**ç­‰ã€‚å¯ä»¥é€šè¿‡ç®€å•çš„é…ç½®ï¼Œè®¾å®šæ¨¡å‹çš„å±‚æ•°ï¼Œhidden_sizeç­‰ã€‚ä½ ä¹Ÿå¯ä»¥æ·±å…¥åˆ°model.pyé‡Œé¢ï¼Œå»å¥‡æ€å¦™æƒ³ï¼Œå†™è‡ªå·±çš„å›¾ç¥ç»ç½‘ç»œã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pgl\n",
    "import model\n",
    "import paddle.fluid as fluid\n",
    "import numpy as np\n",
    "import time\n",
    "from build_model import build_model\n",
    "\n",
    "# # ä½¿ç”¨CPU\n",
    "#place = fluid.CPUPlace()\n",
    "\n",
    "# ä½¿ç”¨GPU\n",
    "place = fluid.CUDAPlace(0)\n",
    "\n",
    "train_program = fluid.default_main_program()\n",
    "startup_program = fluid.default_startup_program()\n",
    "with fluid.program_guard(train_program, startup_program):\n",
    "    with fluid.unique_name.guard():\n",
    "        gw, loss, acc, pred = build_model(dataset,\n",
    "                            config=config,\n",
    "                            phase=\"train\",\n",
    "                            main_prog=train_program)\n",
    "\n",
    "test_program = fluid.Program()\n",
    "with fluid.program_guard(test_program, startup_program):\n",
    "    with fluid.unique_name.guard():\n",
    "        _gw, v_loss, v_acc, v_pred = build_model(dataset,\n",
    "            config=config,\n",
    "            phase=\"test\",\n",
    "            main_prog=test_program)\n",
    "\n",
    "\n",
    "test_program = test_program.clone(for_test=True)\n",
    "\n",
    "exe = fluid.Executor(place)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## å¼€å§‹è®­ç»ƒè¿‡ç¨‹\n",
    "\n",
    "å›¾ç¥ç»ç½‘ç»œé‡‡ç”¨FullBatchçš„è®­ç»ƒæ–¹å¼ï¼Œæ¯ä¸€æ­¥è®­ç»ƒå°±ä¼šæŠŠæ‰€æœ‰æ•´å¼ å›¾è®­ç»ƒæ ·æœ¬å…¨éƒ¨è®­ç»ƒä¸€éã€‚\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "use_label_e = True\n",
    "label_rate = 0.66\n",
    "epoch = 4000\n",
    "exe.run(startup_program)\n",
    "max_val_acc = 0\n",
    "\n",
    "# è¿™é‡Œå¯ä»¥æ¢å¤è®­ç»ƒ\n",
    "pretrained = True\n",
    "if pretrained:\n",
    "    def name_filter(var):\n",
    "        res = var.name in os.listdir('./output')\n",
    "        return res\n",
    "    fluid.io.load_vars(exe, './output',predicate=name_filter)\n",
    "    max_val_acc = 0.63\n",
    "\n",
    "earlystop = 0\n",
    "# å°†å›¾æ•°æ®å˜æˆ feed_dict ç”¨äºä¼ å…¥Paddle Excecutor\n",
    "feed_dict = gw.to_feed(dataset.graph)\n",
    "for epoch in range(epoch):\n",
    "    # Full Batch è®­ç»ƒ\n",
    "    # è®¾å®šå›¾ä¸Šé¢é‚£äº›èŠ‚ç‚¹è¦è·å–\n",
    "    # node_index: æœªçŸ¥labelèŠ‚ç‚¹çš„nid    \n",
    "    # node_label: æœªçŸ¥label\n",
    "    # label_idx: å·²çŸ¥labelèŠ‚ç‚¹çš„nid    \n",
    "    # label: å·²çŸ¥label\n",
    "    \n",
    "    if use_label_e:\n",
    "        # åœ¨è®­ç»ƒé›†ä¸­æŠ½å–éƒ¨åˆ†æ•°æ®ï¼Œå…¶Labelå·²çŸ¥ï¼Œå¹¶å¯ä»¥è¾“å…¥ç½‘ç»œè®­ç»ƒ\n",
    "        train_idx_temp = np.array(train_index, dtype=\"int64\")\n",
    "        train_lab_temp = np.array(train_label, dtype=\"int64\")\n",
    "        state = np.random.get_state()\n",
    "        np.random.shuffle(train_idx_temp)\n",
    "        np.random.set_state(state)\n",
    "        np.random.shuffle(train_lab_temp)\n",
    "\n",
    "        label_idx=train_idx_temp[:int(label_rate*len(train_idx_temp))]\n",
    "        unlabel_idx=train_idx_temp[int(label_rate*len(train_idx_temp)):]\n",
    "        label=train_lab_temp[:int(label_rate*len(train_idx_temp))]\n",
    "        unlabel=train_lab_temp[int(label_rate*len(train_idx_temp)):]\n",
    "\n",
    "        feed_dict[\"node_index\"] = unlabel_idx\n",
    "        feed_dict[\"node_label\"] = unlabel\n",
    "        feed_dict['label_idx']= label_idx\n",
    "        feed_dict['label']= label\n",
    "    else:\n",
    "        feed_dict[\"node_label\"] = np.array(train_label, dtype=\"int64\")\n",
    "        feed_dict[\"node_index\"] = np.array(train_index, dtype=\"int64\")\n",
    "        \n",
    "\n",
    "    train_loss, train_acc = exe.run(train_program,\n",
    "                                feed=feed_dict,\n",
    "                                fetch_list=[loss, acc],\n",
    "                                return_numpy=True)\n",
    "\n",
    "    # Full Batch éªŒè¯\n",
    "    # è®¾å®šå›¾ä¸Šé¢é‚£äº›èŠ‚ç‚¹è¦è·å–\n",
    "    # node_index: æœªçŸ¥labelèŠ‚ç‚¹çš„nid    \n",
    "    # node_label: æœªçŸ¥label\n",
    "    # label_idx: å·²çŸ¥labelèŠ‚ç‚¹çš„nid    \n",
    "    # label: å·²çŸ¥label\n",
    "    \n",
    "    feed_dict[\"node_index\"] = np.array(val_index, dtype=\"int64\")\n",
    "    feed_dict[\"node_label\"] = np.array(val_label, dtype=\"int64\")\n",
    "    if use_label_e:\n",
    "        feed_dict['label_idx'] = np.array(train_index, dtype=\"int64\")\n",
    "        feed_dict['label'] = np.array(train_label, dtype=\"int64\")\n",
    "    val_loss, val_acc = exe.run(test_program,\n",
    "                            feed=feed_dict,\n",
    "                            fetch_list=[v_loss, v_acc],\n",
    "                            return_numpy=True)\n",
    "    print(\"Epoch\", epoch, \"Train Acc\", train_acc[0], \"Valid Acc\", val_acc[0],\"train loss\",train_loss[0],\"val loss\",val_loss[0])\n",
    "    \n",
    "    # ä¿å­˜å†å²æœ€ä¼˜éªŒè¯ç²¾åº¦å¯¹åº”çš„æ¨¡å‹\n",
    "    if val_acc[0] > max_val_acc:\n",
    "        max_val_acc = val_acc[0]\n",
    "        print(val_acc[0])\n",
    "        fluid.io.save_persistables(exe, './output', train_program)\n",
    "    \n",
    "    # è®­ç»ƒç²¾åº¦æŒç»­å¤§äºéªŒè¯ç²¾åº¦ï¼Œç»“æŸè®­ç»ƒ\n",
    "    if train_acc[0] > val_acc[0]:\n",
    "        earlystop += 1\n",
    "        if earlystop == 40:\n",
    "            break\n",
    "    else:\n",
    "        earlystop = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## å¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹\n",
    "\n",
    "è®­ç»ƒå®Œæˆåï¼Œæˆ‘ä»¬å¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹ã€‚é¢„æµ‹çš„æ—¶å€™ï¼Œç”±äºä¸çŸ¥é“æµ‹è¯•é›†åˆçš„æ ‡ç­¾ï¼Œæˆ‘ä»¬éšæ„ç»™ä¸€äº›æµ‹è¯•labelã€‚æœ€ç»ˆæˆ‘ä»¬è·å¾—æµ‹è¯•æ•°æ®çš„é¢„æµ‹ç»“æœã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pretrained = True\r\n",
    "if pretrained:\r\n",
    "    def name_filter(var):\r\n",
    "        res = var.name in os.listdir('./output')\r\n",
    "        return res\r\n",
    "    fluid.io.load_vars(exe, './output',predicate=name_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feed_dict[\"node_index\"] = np.array(test_index, dtype=\"int64\")\n",
    "feed_dict[\"node_label\"] = np.array(test_label, dtype=\"int64\") #å‡æ ‡ç­¾\n",
    "test_prediction = exe.run(test_program,\n",
    "                            feed=feed_dict,\n",
    "                            fetch_list=[v_pred],\n",
    "                            return_numpy=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## ç”Ÿæˆæäº¤æ–‡ä»¶\n",
    "\n",
    "æœ€åä¸€æ­¥ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨pandasè½»æ¾ç”Ÿæˆæäº¤æ–‡ä»¶ï¼Œæœ€åä¸‹è½½ submission.csv æäº¤å°±å¥½äº†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(data={\n",
    "                            \"nid\": test_index.reshape(-1),\n",
    "                            \"label\": test_prediction.reshape(-1)\n",
    "                        })\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def publicnum(num, d = 0):\r\n",
    "    dictnum = {}\r\n",
    "    for i in range(len(num)):\r\n",
    "        if num[i] in dictnum.keys():\r\n",
    "            dictnum[num[i]] += 1\r\n",
    "        else:\r\n",
    "            dictnum.setdefault(num[i], 1)\r\n",
    "    maxnum = 0\r\n",
    "    maxkey = 0\r\n",
    "    for k, v in dictnum.items():\r\n",
    "        if v > maxnum:\r\n",
    "            maxnum = v\r\n",
    "            maxkey = k\r\n",
    "    return maxkey\r\n",
    "\r\n",
    "df0=pd.read_csv(\"submission0.76136.csv\")\r\n",
    "df1=pd.read_csv(\"submission0.757822.csv\")\r\n",
    "df2=pd.read_csv(\"submission0.7583.csv\")\r\n",
    "df3=pd.read_csv(\"submission0.75758.csv\")\r\n",
    "df4=pd.read_csv(\"submission0.75921.csv\")\r\n",
    "df5=pd.read_csv(\"submission0.75782.csv\")\r\n",
    "df6=pd.read_csv(\"submission0.75956.csv\")\r\n",
    "df7=pd.read_csv(\"submission0.75801.csv\")\r\n",
    "df8=pd.read_csv(\"submission0.75884.csv\")\r\n",
    "#df9=pd.read_csv(\"submission9.csv\")\r\n",
    "#df10=pd.read_csv(\"submission10.csv\")\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "nids=[]\r\n",
    "labels=[]\r\n",
    "\r\n",
    "for i in range(df4.shape[0]):\r\n",
    "    label_zs=[]\r\n",
    "    label_zs.append(df0.label[i])\r\n",
    "    label_zs.append(df1.label[i])\r\n",
    "    label_zs.append(df2.label[i])\r\n",
    "    label_zs.append(df3.label[i])\r\n",
    "    label_zs.append(df4.label[i])\r\n",
    "    label_zs.append(df5.label[i])\r\n",
    "    label_zs.append(df6.label[i])\r\n",
    "    label_zs.append(df7.label[i])\r\n",
    "    label_zs.append(df8.label[i])\r\n",
    "    #label_zs.append(df9.label[i])\r\n",
    "    #label_zs.append(df10.label[i])\r\n",
    "    lab=publicnum(label_zs, d = 0)\r\n",
    "    labels.append(lab)\r\n",
    "    nids.append(df4.nid[i])\r\n",
    "\r\n",
    "\r\n",
    "submission = pd.DataFrame(data={\r\n",
    "                            \"nid\": nids,\r\n",
    "                            \"label\": labels\r\n",
    "                        })\r\n",
    "submission.to_csv(\"submissiona.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 1.8.4 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
